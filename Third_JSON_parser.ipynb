{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Parser: Selecting a Passage\n",
    "by Niek Veldhuis\n",
    "UC Berkeley\n",
    "\n",
    "July 2017 2017\n",
    "\n",
    "Main difference between this parser and the `Second_JSON_parser.ipynb` is the ability to define a region within a tablet to parse by giving a start label and/or a stop label. This may be used in order to omit colophons or to select one exercise among several on a school tablet.\n",
    "\n",
    "This requires some extra parsing of the list of text IDs where labels are now optional. The list of text IDs may look like this:\n",
    "\n",
    "- dcclt/Q000039\n",
    "- dcclt/P247864 - r i 16\n",
    "- dcclt/P228683 r 1 - r 4\n",
    "- dcclt/P230849 r 1 -\n",
    "\n",
    "The first will parse the entirety of `Q000039`; the second will take everything of `P247864` up to and including `r i 16`. The third will only take `r 1` to (and including) `r 4` of `P228683`. The fourth will parse everything of `P230849` starting from `r 1`. The format of the labels depends on the structure of the text object and the way it has been edited. The labels must follow exactly the form they have in the online [ORACC](http://oracc.org) edition.\n",
    "\n",
    "## Licensing\n",
    "This notebook may be downloaded, used and adapted without any restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import requests\n",
    "import zipfile\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = input('Filename: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('text_ids/' + name, 'r') as f:\n",
    "    pqxnos = f.read().splitlines()\n",
    "pqxnos = [no.strip() for no in pqxnos]             # strip spaces left and right\n",
    "nos_labels = [no.split(' ', 1) if \" \" in no else [no, '-'] for no in pqxnos] # isolate text ID\n",
    "for label in nos_labels:    # separate start label and stop label\n",
    "    label[1] = label[1].split('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Parse JSON files\n",
    "The `parsejson()` below includes `id_word` which has the form `TextID.LineID.WordID` - in other words, line and text ID can be derived from it.\n",
    "\n",
    "Parsejson() takes as second argument a logical variable. If \"True\" the parser starts with the first word.\n",
    "If \"False\" the parser starts when it gets to \"startlabel\". The parser stops when it gets to \"endlabel\". `Label` `startlabel` and `stoplabel` are stored in the dictionary `labels` outside of the function.\n",
    "\n",
    "The list `dollar_keys` (also outside of the function) stores the relevant field names when capturing line breaks etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsejson(text, keep = False):\n",
    "    for dictionary in text[\"cdl\"]:\n",
    "        if \"cdl\" in dictionary: \n",
    "            parsejson(dictionary, keep)\n",
    "        if \"label\" in dictionary:\n",
    "            labels[\"label\"] = dictionary[\"label\"]\n",
    "        if labels[\"label\"] == labels[\"startlabel\"]:\n",
    "            keep = True\n",
    "        if labels[\"label\"] == labels[\"endlabel\"]:\n",
    "            keep = False\n",
    "        if keep == True or labels[\"label\"] == labels[\"endlabel\"]: # the \"or\" statement ensures that the line\n",
    "            if \"f\" in dictionary:             # corresponding to the endlabel is included.\n",
    "                lemma = dictionary[\"f\"]\n",
    "                lemma[\"id_word\"] = dictionary[\"ref\"]\n",
    "                lemma[\"label\"] = labels[\"label\"]\n",
    "                lemm_l.append(lemma)\n",
    "            if \"strict\" in dictionary and dictionary[\"strict\"] == \"1\":\n",
    "                lemma = {key: dictionary[key] for key in dollar_keys}\n",
    "                lemma[\"id_word\"] = dictionary[\"ref\"] + \".0\"\n",
    "                lemm_l.append(lemma)\n",
    "    return lemm_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.6 Call the Parser for each Textid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemm_l = []\n",
    "dollar_keys = [\"extent\", \"scope\", \"state\"]\n",
    "for pqx in tqdm.tqdm(nos_labels):\n",
    "    project = pqx[0][:-8]\n",
    "    textid = pqx[0][-7:]\n",
    "    labels = {\"startlabel\":pqx[1][0].strip(), \"endlabel\":pqx[1][1].strip(), \"label\":\"\"}\n",
    "    if labels[\"startlabel\"] == \"\":\n",
    "        keep = True\n",
    "    else:\n",
    "        keep = False\n",
    "    url = \"http://oracc.museum.upenn.edu/\" + project + \"/corpusjson/\" + textid + \".json\"  \n",
    "    r = requests.get(url).json()\n",
    "    try:\n",
    "        lemm_l = parsejson(r, keep)\n",
    "    except:\n",
    "        print(url + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Transform the Data into a DataFrame\n",
    "The word_l list is transformed into a Pandas dataframe for further manipulation.\n",
    "\n",
    "For various reasons not all JSON files will have all data types that potentially exist in an [ORACC](http://oracc.org) signature. Only Sumerian words have a `base`, so if your data set has no Sumerian, this column will not exist in the DataFrame.  If a text has no breakage information in the form of `$ 1 line broken` (etc.) the fields `extent`, `scope`, and `state` do not exist. Since such fields are referenced in the code below (sections 2-4) the next cell will check for the existence of each column and create an empty column if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = pd.DataFrame(lemm_l)\n",
    "fields = ['base', 'cf', 'cont', 'epos', 'extent', 'form', 'gw', 'id_word',\n",
    "          'label', 'lang', 'morph', 'norm', 'norm0', 'pos', 'scope', 'sense', 'sig']\n",
    "for field in fields:\n",
    "    if not field in words.columns:\n",
    "        words[field] = ''\n",
    "words = words.fillna('') # replace Missing Values by empty string\n",
    "words.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Remove Spaces and Commas from Guide Word and Sense\n",
    "Spaces in Guide Word and Sense may cause trouble in computational methods in tokenization, or when saved in Comma Separated Values format. All spaces and commas are replaced by hyphens or nothing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words['sense'] = [x.replace(' ', '-') for x in words['sense']]\n",
    "words['sense'] = [x.replace(',', '') for x in words['sense']]\n",
    "words['gw'] = [x.replace(' ', '-') for x in words['gw']]\n",
    "words['gw'] = [x.replace(',', '') for x in words['gw']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the resulting DataFrame correspond to the elements of a full [ORACC](http://oracc.org) signature, plus information about text, line, and word ids:\n",
    "* base (Sumerian only)\n",
    "* cf (Citation Form)\n",
    "* cont (continuation of the base; Sumerian only)\n",
    "* epos (Effective Part of Speech)\n",
    "* form (transliteration, omitting all flags such as indication of breakage)\n",
    "* frag (transliteration; including flags)\n",
    "* gdl_utf8 (cuneiform)\n",
    "* gw (Guide Word: main or first translation in standard dictionary)\n",
    "* id_line (a line ID that begins with the six-digit P, Q, or X number of the text)\n",
    "* id_text (six-digit P, Q, or X number)\n",
    "* id_word (word ID that begins with the ID number of the line)\n",
    "* label (traditional line number in the form o ii 2' (obverse column 2 line 2'), etc.)\n",
    "* lang (language code, including sux, sux-x-emegir, sux-x-emesal, akk, akk-x-stdbab, etc)\n",
    "* morph (Morphology; Sumerian only)\n",
    "* norm (Normalization: Akkadian)\n",
    "* norm0 (Normalization: Sumerian)\n",
    "* pos (Part of Speech)\n",
    "* sense (contextual meaning)\n",
    "* sig (full ORACC signature)\n",
    "\n",
    "Not all data elements (columns) are available for all words. Sumerian words never have a `norm`, Akkadian words do not have `norm0`, `base`, `cont`, or `morph`. Most data elements are only present when the word is lemmatized; only `lang`, `form`, `pos`, `id_word`, `id_line`, and `id_text` should always be there. An unlemmatized word has `pos` 'X' (for unknown). Broken words have `pos` 'u' (for 'unlemmatizable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Manipulate for Analysis on Line level (e.g. phylogenetics)\n",
    "For analyses that use a line as unit of analysis (e.g. lines in lexical texts as analyzed in the [Phylogenetics](https://github.com/ErinBecker/digital-humanities-phylogenetics) project) one may need to create lemmas and combine these into lines by using the `id_line` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create Lemmas and Adjust Bases\n",
    "A lemma, [ORACC](http://oracc.org) style, combines Citation Form, GuideWord and POS into a unique reference to one particular lemma in a standard dictionary, as in `lugal[king]N` (Sumerian) or `šarru[king]N`. Usually, not all words in a text are lemmatized, because a word may be (partly) broken and/or unknown. Unlemmatized and unlemmatizable words will receive a place-holder lemmatization that consists of the transliteration of the word (instead of the Citation Form), with `NA` as GuideWord and POS, as in `i-bu-x[NA]NA`. Note that `NA` is a string.\n",
    "\n",
    "For Sumerian projects each lemmatized word has a `base` (the word without morphology). For non-lemmatized words a place-holder base is created that consists of the transliteration of the word. If you are not working with Sumerian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words[\"lemma\"] = words.apply(lambda r: (r[\"cf\"] + '[' + r[\"gw\"] + ']' + r[\"pos\"]) \n",
    "                            if r[\"cf\"] != '' else r['form'] + '[NA]NA', axis=1)\n",
    "words['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in words['lemma'] ]\n",
    "words['base'] = words.apply(lambda r: r[\"base\"] if r[\"base\"] != '' or r['label'] == '' else r['form'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Group by Line\n",
    "In the `words` dataframe each word has a separate row. In order to change this into a line-by-line representation we use the Pandas `.groupby` function, using the `id_line`, `id_text` and `label` fields as arguments. `label` is a human-readable line number in the format `o ii 3`: obverse column 2, line 3. The field `id_line` is an integer that is created from `id_word`, which has the format `IDText.IDLine.IDWord`, for instance `P296528.23.1`. The field `id_text` is also derived from `id_word`.\n",
    "\n",
    "The fields that are aggregated are `lemma`, `base`, `extent`, and `scope`. The fields `extent` and `scope` represent data on the number of broken lines. If you work with Akkadian data you want to leave out the field `base`.\n",
    "\n",
    "## Note:\n",
    "Make sure that code handles `id_line` that includes `l`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words['id_line'] = [int(wordid[wordid.find('.')+1:wordid.rfind('.')]) for wordid in words['id_word']]\n",
    "words['id_text'] = [wordid[:7] for wordid in words['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = words.groupby([words['id_text'], words['id_line'], words['label']]).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'base': ' '.join,\n",
    "        'extent': ''.join, \n",
    "        'scope': ''.join\n",
    "    }).reset_index()\n",
    "lines        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the new `id_line` field is not a line number in the traditional sense of the word (this is `label`) but a number used to organize lines in the appropriate order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Save in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = name[:-4]\n",
    "with open('output/' + filename + '.csv', 'w') as w:\n",
    "    lines.to_csv(w, encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
