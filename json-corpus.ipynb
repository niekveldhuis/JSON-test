{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing ORACC Data from corpus.json\n",
    "by Niek Veldhuis\n",
    "\n",
    "February 2017\n",
    "\n",
    "# TODO\n",
    "* check that COFs are treated properly\n",
    "* check that lines that continue into the next line (as in bilinguals) are captured completely.\n",
    "* look for `'$ n lines broken'` lines. \n",
    "\n",
    "'$' line has 'node' = 'd' with 'type' = 'nonx' and 'strict' = '1'. There are three content nodes: 'extent', 'scope' and 'state'.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Purpose of the code is to download [ORACC](http://oracc.org) JSON files that contain textual data and produce a `.csv` file with the relevant data for use in computational text analysis. This comes in the place of scraping the published `html`. The JSON files contain all the transliteration and lemmatization data of an ORACC project (metadata are made available in a separate `.json` file).\n",
    "\n",
    "The resulting data file may include various elements of the ORACC data structure. The current code will output a file with the following fields: \n",
    "* textid\n",
    "* line number\n",
    "* lemmatization\n",
    "* bases\n",
    "\n",
    "The selection of fields may be adjusted with standard `Pandas` functions.\n",
    "\n",
    "## Notes\n",
    "The current version of the script works with the `ijson` library. Documentation for [ijson](https://www.dataquest.io/blog/python-json-tutorial/), unfortunately, is extremely brief. This notebook is written for exploratory purposes, using a list of 106 P, Q, X numbers (in `ob_lists_wood.txt`). Downloading the `.json` files takes between 30 and 45 seconds. The rest of the script is reasonably fast. With larger lists of text IDs the script will obviously take longer. \n",
    "\n",
    "This notebook is written for **Python 3.5** with **Pandas 0.19** and **ijson 2.3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "import urllib.request\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q000039', 'P117395', 'P117404', 'P128345', 'P224980']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textids = 'text_ids/ob_lists_wood.txt'\n",
    "with open(textids, 'r') as f:\n",
    "    pnos = f.readlines()\n",
    "pnos = [x.strip() for x in pnos]\n",
    "pnos = [x[-7:] for x in pnos]\n",
    "pnos[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse\n",
    "The function `oraccjasonparser()` takes one argument (the **url** of the `.json` file). It looks for the prefix `textid` to retrieve the six-digit P, Q, or X number of the text artifact. Parsing the file sequentially the code looks for the places where a line starts (`'.type' = 'line-start'`) and where a word starts (`'.node' = 'l'`). At each level the code will retrieve the relevant data and create a list where each entry is a dictionary that represents a single word. \n",
    "\n",
    "Words not only included lemmatized words, but also unlemmatized and unlemmatizable words (such as breaks).\n",
    "\n",
    "The dictionary includes the keys `id_line` and `id_word` that allow the user to reassemble words and lines in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oraccjasonparser(url):\n",
    "    d = urllib.request.urlopen(url)\n",
    "    parser = ijson.parse(d)\n",
    "    word_l = []\n",
    "    word_d = {}\n",
    "    line_start = False\n",
    "    word_start = False\n",
    "    for prefix, event, value in parser:\n",
    "        if prefix == 'textid':\n",
    "            id_text = value\n",
    "#            print(\"parsing \" + value)\n",
    "        if prefix.endswith('.type'):\n",
    "            if value == 'line-start':\n",
    "                line_start = True\n",
    "            else:\n",
    "                line_start = False\n",
    "        if line_start:\n",
    "            if prefix.endswith('.ref') and not word_start:\n",
    "                id_line = value\n",
    "            if prefix.endswith('.label'):\n",
    "                label = value\n",
    "        if prefix.endswith('node'):\n",
    "            if value == 'l':\n",
    "                word_start = True\n",
    "                if not word_d == {}:\n",
    "                    word_l.append(word_d)\n",
    "                word_d = {}\n",
    "                word_d['id_text'] = id_text\n",
    "                word_d['id_line'] = id_line\n",
    "                word_d['label'] = label\n",
    "            else:\n",
    "                word_start = False\n",
    "        if word_start:\n",
    "            if prefix.endswith('.ref'):\n",
    "                word_d['id_word'] = value\n",
    "            if prefix.endswith('.sig'):\n",
    "                word_d['signature'] = value\n",
    "            if '.f.' in prefix:\n",
    "                category = re.sub('.*\\.', '', prefix) # get element after the last dot of the prefix\n",
    "                word_d[category] = value\n",
    "    word_l.append(word_d)\n",
    "    return(word_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the Parser Function for Each Textid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 22/106 [00:13<00:47,  1.77it/s]"
     ]
    }
   ],
   "source": [
    "url_prefix = \"http://oracc.museum.upenn.edu/dcclt/corpusjson/\"\n",
    "word_l = []\n",
    "for id_text in tqdm.tqdm(pnos):\n",
    "    url = url_prefix + id_text + '.json'\n",
    "    word_l.extend(oraccjasonparser(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the Data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_l)\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Spaces and Commas from Guide Word and Sence\n",
    "Spaces in Guide Word and Sense may cause trouble in computational methods in tokenization, or when saved in Comma Separated Values format. All spaces and commas are replaced by hyphens or nothing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = words.fillna('') # first replace Missing Values by empty string\n",
    "words['sense'] = [x.replace(' ', '-') for x in words['sense']]\n",
    "words['sense'] = [x.replace(',', '') for x in words['sense']]\n",
    "words['gw'] = [x.replace(' ', '-') for x in words['gw']]\n",
    "words['gw'] = [x.replace(',', '') for x in words['gw']]\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the resulting DataFrame correspond to the elements of a full [ORACC](http://oracc.org) signature, plus information about text, line, and word ids:\n",
    "* base (Sumerian only)\n",
    "* cf (Citation Form)\n",
    "* cont (continuation of the base; Sumerian only)\n",
    "* epos (Effective Part of Speech)\n",
    "* form (transliteration, omitting all flags such as indication of breakage)\n",
    "* gw (Guide Word: main or first translation in standard dictionary)\n",
    "* id_line (a line ID that begins with the six-digit P, Q, or X number of the text)\n",
    "* id_text (six-digit P, Q, or X number)\n",
    "* id_word (word ID that begins with the ID number of the line)\n",
    "* label (traditional line number in the form o ii 2' (obverse column 2 line 2'), etc.)\n",
    "* lang (language code, including sux, sux-x-emegir, sux-x-emesal, akk, akk-x-stdbab, etc)\n",
    "* morph (Morphology; Sumerian only)\n",
    "* norm (Normalization: Akkadian)\n",
    "* norm0 (Normalization: Sumerian)\n",
    "* pos (Part of Speech)\n",
    "* sense (contextual meaning)\n",
    "* signature (full ORACC signature)\n",
    "\n",
    "Not all data elements (columns) are available for all words. Sumerian words never have a `norm`, Akkadian words do not have `norm0`, `base`, `cont`, or `morph`. Most data elements are only present when the word is lemmatized; only `lang`, `form`, `pos`, `id_word`, `id_line`, and `id_text` should always be there. An unlemmatized word has `pos` 'X' (for unknown). Broken words have `pos` 'u' (for 'unlemmatizable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate\n",
    "The columns may be manipulated with standard Pandas methods to create the desired output. By way of example, the following code will create a column `lemma` with the format **cf[gw]pos** (for instance **lugal[king]N**). For words that have no lemmatization `lemma` equals `form`. Only Sumerian words are allowed (and thus `lang` can be omitted) and in addition to the column `lemma` the column `base` is preserved; words that have no lemmatization take `form` as their base. Words and bases are concatenated to lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove  non-Sumerian words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = words.loc[words['lang'].str[:3] == 'sux'].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lemma Column and Adjust Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words['lemma'] = words['cf'] # first element of lemma is the citation form\n",
    "words['lemma'] = [words['lemma'][i] + '[' + \n",
    "                  words['gw'][i] + ']' +\n",
    "                  words['pos'][i] if not words['lemma'][i] == '' else words['form'][i] for i in range(len(words))]\n",
    "words['base'] = [words['base'][i] if not words['base'][i] == '' else words['form'][i] for i in range(len(words))]\n",
    "lemmas = words[['lemma', 'base', 'id_text', 'id_line', 'id_word', 'label']]\n",
    "lemmas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = words.groupby([lemmas['id_line'], lemmas['label']]).agg({'lemma': ' '.join, 'base': ' '.join}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('output/obwood.cvs', 'w') as w:\n",
    "    lines.to_csv(w, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
