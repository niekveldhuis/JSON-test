{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Parser: Lines and Breaks\n",
    "by Niek Veldhuis\n",
    "UC Berkeley\n",
    "\n",
    "July 2017\n",
    "\n",
    "\n",
    "# TODO\n",
    "* check that COFs are treated properly\n",
    "* check that lines that continue into the next line (as in bilinguals) are captured completely. Such lines are indicated in the json by the the addition of 'l' (lower case L) to the reference (.ref).\n",
    "* clean up Notebook\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The two main differences between `First_JSON_parser.ipynb` and the current notebook are\n",
    "\n",
    "- the ability to parse an entire corpus\n",
    "- recognizing lines\n",
    "- including breaks (as in \"3 lines broken\").\n",
    "\n",
    "Although these features somewhat complicate the code, the basic techniques used are the same.\n",
    "\n",
    "The resulting data file may include various elements of the ORACC data structure. The current code will output a file with the following fields: \n",
    "\n",
    "* id_line\n",
    "* label\n",
    "* lemma\n",
    "* base\n",
    "* extent\n",
    "* scope\n",
    "\n",
    "The fields `extent` and `scope` capture the number of missing lines or columns.\n",
    "\n",
    "The selection of fields may be adjusted with standard `Pandas` functions.\n",
    "\n",
    "## Notes\n",
    "\n",
    "This notebook is written for **Python 3.5** with **Pandas 0.19** and **requests 2.12.4**.\n",
    "\n",
    "\n",
    "## Licensing\n",
    "This notebook may be downloaded, used and adapted without any restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import requests\n",
    "import zipfile\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input List of Text IDs or a project abbreviation\n",
    "Identify a list of text IDs (P, Q, and X numbers) in the directory `input`. The IDs are six-digit P, Q, or X numbers preceded by a project abbreviation in the format 'PROJECT/P######' or 'PROJECT/SUBPROJECT/Q######'. For example:\n",
    "* dcclt/P117395\n",
    "* etcsri/Q001203\n",
    "* rinap/rinap1/Q003421\n",
    "\n",
    "The list should be created with a flat text editor such as Textedit or Emacs, and the filename should end in `.txt`.\n",
    "\n",
    "Alternatively, one may enter the name (abbreviation) of a project or sub-project in [ORACC](http://oracc.org) and pull all the lemmatized data from that project. Note that the script will not automatically pull data from subprojects, they have to be requested separately. Examples:\n",
    "* saao/saa01\n",
    "* aemw/amarna\n",
    "* rimanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = input('Filename or project abbreviation: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if name[-4:] == '.txt':\n",
    "    textids = 'text_ids/' + name\n",
    "    with open(textids, 'r') as f:\n",
    "        pqxnos = f.readlines()\n",
    "    pqxnos = [x.strip() for x in pqxnos]  # strip spaces left and right\n",
    "    pqxnos = [x for x in pqxnos if not x == \"\"] # strip empty lines\n",
    "#    pqxnos = [x[-7:].upper() for x in pqxnos]\n",
    "    projects = [x[:-8].lower() for x in pqxnos]\n",
    "    projects = list(set(projects))\n",
    "else:\n",
    "    name = name.strip().lower()\n",
    "    projects = [name]\n",
    "    url = \"http://oracc.org/\" + name + \"/corpus.json\"\n",
    "    r = requests.get(url)\n",
    "    corpus = r.json()\n",
    "    pqxnos = list(corpus[\"members\"].keys())\n",
    "    pqxnos = [name + '/' + no for no in pqxnos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create Download Directory and JSON directory\n",
    "For the code, see [Stack Overflow](http://stackoverflow.com/questions/18973418/os-mkdirpath-returns-oserror-when-directory-does-not-exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import errno\n",
    "import os\n",
    "try:\n",
    "    os.mkdir('jsonzip')\n",
    "except OSError as exc:\n",
    "    if exc.errno !=errno.EEXIST:\n",
    "        raise\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir('json')\n",
    "except OSError as exc:\n",
    "    if exc.errno !=errno.EEXIST:\n",
    "        raise\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Download `json.zip`\n",
    "For each project from which files are to be processed download the entire project (all the json files) in `https://github.com/oracc/json`. The file is called `PROJECT.zip` (for instance: `dcclt.zip`). For subprojects the file is called `PROJECT-SUBPROJECT.zip` (for instance `cams-gkab.zip`). \n",
    "\n",
    "For larger projects (such as [DCCLT](http://oracc.org/dcclt)) the `zip` file may be 25Mb or more. Downloading may take some time and it may be necessary to chunk the downloading process. The `iter_content()` function in the `requests` library takes care of that.\n",
    "\n",
    "Although downloading the entire zip file is time consuming, it will make processing the individual files much more efficient and the code is less likely to break due to interruption in connectivity.\n",
    "\n",
    "## Note:\n",
    "It may be better to download the `zip` file from [ORACC](http://oracc.org), where it is available as `http://oracc.org/[PROJECT]/json.zip`. This version is updated when a project is updated. Right now the file seems to be not accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHUNK = 16 * 1024\n",
    "for project in tqdm.tqdm(projects):\n",
    "    project = project.replace('/', '-')\n",
    "    url = \"https://raw.github.com/oracc/json/master/\" + project + \".zip\"\n",
    "    file = 'jsonzip/' + project + '.zip'\n",
    "    print(\"Downloading \" + url + \" saving as \" + file)\n",
    "    r = requests.get(url)\n",
    "    with open(file, 'wb') as f:\n",
    "        for c in r.iter_content(chunk_size=CHUNK):\n",
    "            f.write(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Extract JSON files from `json.zip`\n",
    "Extract the texts listed in the list of text IDs from the `json.zip`. All files are extracted to a directory called `data/[PROJECT]/json/corpusjson` (for instance `data/dcclt/json/corpusjson`). If the file belongs to a subproject the directory is called `data/[PROJECT]/[SUBPROJECT]/json/corpusjson`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_dir = 'json'\n",
    "for no in tqdm.tqdm(pqxnos):\n",
    "    project = no[:-8].lower()\n",
    "    pno = no[-7:].upper()\n",
    "    zip_file = \"jsonzip/\" + project.replace('/', '-') + \".zip\"\n",
    "    with zipfile.ZipFile(zip_file,\"r\") as zip_ref:\n",
    "        file = project + '/corpusjson/' + pno + '.json'\n",
    "        try:\n",
    "            zip_ref.extract(file, target_dir)\n",
    "        except:\n",
    "            print(no + ' is not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Parse JSON files\n",
    "The `parsejson()` function is essentially identical with the that function in `First_JSON_parser.ipynb`, but it fetches more data. The field `word_id` consists of three parts, namely a text ID, line ID, and word ID, in the format `Q000039.76.2` meaning: the second word in line 76 of text object `Q000039`. Note that `76` is not a line number strictly speaking but an object reference within the text object. Things like horizontal rulings, columns, and breaks also get object references. The `word_id` field allows us to put lines together in the proper order.\n",
    "\n",
    "The field `label` is a human-legible label that refers a line or another part of the text; it may look like `o i 23` (obverse column 1 line 23) or `r v 23'` (reverse column 5 line 23 prime). The `label` field is used in online [ORACC](http://oracc.org) editions to indicate line numbers.\n",
    "\n",
    "The fields `extent`, `scope`, and `state` give metatextual data about the condition of the object; they capture the number of broken lines or columns and similar information. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsejson(text, lemm_l = None):\n",
    "    dollar_keys = [\"extent\", \"scope\",\"state\"]\n",
    "    if lemm_l == None:\n",
    "        lemm_l = []\n",
    "    for dictionary in text[\"cdl\"]:\n",
    "        if \"cdl\" in dictionary: \n",
    "            parsejson(dictionary, lemm_l)\n",
    "        if \"label\" in dictionary:\n",
    "            label = dictionary['label']\n",
    "        if \"f\" in dictionary:\n",
    "            lemma = dictionary[\"f\"]\n",
    "            lemma[\"id_word\"] = dictionary[\"ref\"]\n",
    "            lemma['label'] = label\n",
    "            lemm_l.append(lemma)\n",
    "        if \"strict\" in dictionary and dictionary[\"strict\"] == \"1\":\n",
    "            lemma = {key: dictionary[key] for key in dollar_keys}\n",
    "            lemma[\"id_word\"] = dictionary[\"ref\"] + \".0\"\n",
    "            lemm_l.append(lemma)\n",
    "    return lemm_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Call the Parser Function for Each Textid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_l = []\n",
    "for id_text in tqdm.tqdm(pqxnos):\n",
    "    try:\n",
    "        word_l.extend(oraccjsonparser(id_text))\n",
    "    except:\n",
    "        print(no + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Transform the Data into a DataFrame\n",
    "The word_l list is transformed into a Pandas dataframe for further manipulation.\n",
    "\n",
    "For various reasons not all JSON files will have all data types that potentially exist in an [ORACC](http://oracc.org) signature. Only Sumerian words have a `base`, so if your data set has no Sumerian, this column will not exist in the DataFrame.  If a text has no breakage information in the form of `$ 1 line broken` (etc.) the fields `extent`, `scope`, and `state` do not exist. Since such fields are referenced in the code below (sections 2-4) the next cell will check for the existence of each column and create an empty column if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_l)\n",
    "fields = ['base', 'cf', 'cont', 'epos', 'extent', 'form', 'gw', 'id_word',\n",
    "          'label', 'lang', 'morph', 'norm', 'norm0', 'pos', 'scope', 'sense', 'sig']\n",
    "for field in fields:\n",
    "    if not field in words.columns:\n",
    "        words[field] = ''\n",
    "words = words.fillna('') # replace Missing Values by empty string\n",
    "words.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Remove Spaces and Commas from Guide Word and Sense\n",
    "Spaces in Guide Word and Sense may cause trouble in computational methods in tokenization, or when saved in Comma Separated Values format. All spaces and commas are replaced by hyphens or nothing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words['sense'] = [x.replace(' ', '-') for x in words['sense']]\n",
    "words['sense'] = [x.replace(',', '') for x in words['sense']]\n",
    "words['gw'] = [x.replace(' ', '-') for x in words['gw']]\n",
    "words['gw'] = [x.replace(',', '') for x in words['gw']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the resulting DataFrame correspond to the elements of a full [ORACC](http://oracc.org) signature, plus information about text, line, and word ids:\n",
    "* base (Sumerian only)\n",
    "* cf (Citation Form)\n",
    "* cont (continuation of the base; Sumerian only)\n",
    "* epos (Effective Part of Speech)\n",
    "* form (transliteration, omitting all flags such as indication of breakage)\n",
    "* frag (transliteration; including flags)\n",
    "* gdl_utf8 (cuneiform)\n",
    "* gw (Guide Word: main or first translation in standard dictionary)\n",
    "* id_line (a line ID that begins with the six-digit P, Q, or X number of the text)\n",
    "* id_text (six-digit P, Q, or X number)\n",
    "* id_word (word ID that begins with the ID number of the line)\n",
    "* label (traditional line number in the form o ii 2' (obverse column 2 line 2'), etc.)\n",
    "* lang (language code, including sux, sux-x-emegir, sux-x-emesal, akk, akk-x-stdbab, etc)\n",
    "* morph (Morphology; Sumerian only)\n",
    "* norm (Normalization: Akkadian)\n",
    "* norm0 (Normalization: Sumerian)\n",
    "* pos (Part of Speech)\n",
    "* sense (contextual meaning)\n",
    "* sig (full ORACC signature)\n",
    "\n",
    "Not all data elements (columns) are available for all words. Sumerian words never have a `norm`, Akkadian words do not have `norm0`, `base`, `cont`, or `morph`. Most data elements are only present when the word is lemmatized; only `lang`, `form`, `pos`, `id_word`, `id_line`, and `id_text` should always be there. An unlemmatized word has `pos` 'X' (for unknown). Broken words have `pos` 'u' (for 'unlemmatizable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Manipulate for SNA\n",
    "The columns of the `words` DataFrame may be manipulated with standard Pandas methods to create the desired output. By way of example, the following code will select proper nouns only and create two `.csv` files (`edges.csv` and `nodes.csv`) that may be ingested by Social Network Analysis (SNA) software. The column names follow the conventions used in [Gephi](https://gephi.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Select Proper Nouns\n",
    "First list all Part of Speech tags currently available in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = list(set(words['pos']))\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then list the tags that are relevant in the list `pos` and use that list to select the rows of the DataFrame that contain proper nouns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = ['CN', 'DN', 'EN', 'FN', 'GN', 'PN', 'NN', 'RN', 'SN', 'TN', 'WN'] # what is 'NN'?\n",
    "proper_nouns = words.loc[words['pos'].isin(pos)].reset_index(drop=True)\n",
    "proper_nouns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Keep  Norm, Pos, and id_text\n",
    "Now select the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "proper_nouns = proper_nouns[['norm', 'pos', 'id_text']].drop_duplicates()\n",
    "proper_nouns = proper_nouns[proper_nouns['norm'] != ''].reset_index(drop=True)\n",
    "proper_nouns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Create Edge List\n",
    "The edge list contains the columns `source` and `target` and combines all proper nouns that appear in a single text as source-target pairs. All edges are considered `undirected`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edges = []\n",
    "for i in tqdm.tqdm(range(len(proper_nouns))):\n",
    "    for j in range(i+1, len(proper_nouns)):\n",
    "        if proper_nouns['id_text'][i] == proper_nouns['id_text'][j]:\n",
    "            edge = [proper_nouns['norm'][i], proper_nouns['norm'][j]]\n",
    "            edges.append(edge)\n",
    "        else:\n",
    "            break\n",
    "edges_df = pd.DataFrame(edges)\n",
    "edges_df.columns = ['source', 'target']\n",
    "edges_df['type'] = 'undirected'\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"output/edges.csv\", 'w') as f:\n",
    "    edges_df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Create Node List\n",
    "pn_set contains the unique proper nouns in the entire corpus. This become the node list in Gephi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pn_set = proper_nouns[['norm', 'pos']].drop_duplicates() # Assur DN and Assur GN are not considered duplicates!\n",
    "pn_set.columns = ['Id', 'Type']\n",
    "pn_set['Label'] = pn_set['Id']\n",
    "pn_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"output/nodes.csv\", 'w') as f:\n",
    "    pn_set.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Manipuate for Analysis on Line level (e.g. phylogenetics)\n",
    "For analyses that use a line as unit of analysis (e.g. lines in lexical texts as analyzed in the [Phylogenetics](https://github.com/ErinBecker/digital-humanities-phylogenetics) project) one may need to create lemmas and combine these into lines by using the `id_line` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create Lemmas and Adjust Bases\n",
    "A lemma, [ORACC](http://oracc.org) style, combines Citation Form, GuideWord and POS into a unique reference to one particular lemma in a standard dictionary, as in `lugal[king]N` (Sumerian) or `šarru[king]N`. Usually, not all words in a text are lemmatized, because a word may be (partly) broken and/or unknown. Unlemmatized and unlemmatizable words will receive a place-holder lemmatization that consists of the transliteration of the word (instead of the Citation Form), with `NA` as GuideWord and POS, as in `i-bu-x[NA]NA`. Note that `NA` is a string.\n",
    "\n",
    "For Sumerian projects each lemmatized word has a `base` (the word without morphology). For non-lemmatized words a place-holder base is created that consists of the transliteration of the word. If you are not working with Sumerian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words[\"lemma\"] = words.apply(lambda r: (r[\"cf\"] + '[' + r[\"gw\"] + ']' + r[\"pos\"]) \n",
    "                            if r[\"cf\"] != '' else r['form'] + '[NA]NA', axis=1)\n",
    "words['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in words['lemma'] ]\n",
    "words['base'] = words.apply(lambda r: r[\"base\"] if r[\"base\"] != '' or r['label'] == '' else r['form'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Group by Line\n",
    "In the `words` dataframe each word has a separate row. In order into change this to a line-by-line representation we use the Pandas `.groupby` function, using the `id_line` and `label` fields as arguments (`id_line` has an abstract number that indicates the sequence of lines in a text object; `label` is a human-readable line number in the format `o ii 3`: obverse column 2, line 3). The fields that are aggregated are `lemma`, `base`, `extent`, and `scope`. The fields `extent` and `scope` represent data on the number of broken lines. If you work with Akkadian data you want to leave out the field `base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words['id_line'] = [wordid[:wordid.rfind('.')+1] for wordid in words['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = words.groupby([words['id_line'], words['label']]).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'base': ' '.join,\n",
    "        'extent': ''.join, \n",
    "        'scope': ''.join\n",
    "    }).reset_index()\n",
    "lines        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `id_line` is a string variable and therefore does not give the lines in the right order. We should split `id_line` into two variables: `id_text` (the first 7 characters; we lost the old `id_text` column in the `.groupby` function above) and a new `line` variable, which is a number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines['id_text'] = lines['id_line'].str[:7] # id_text was lost in the grouping above and is recreated\n",
    "lines['line'] = [line[line.rfind('.')+1:] for line in lines['id_line']] #create a line number for sorting\n",
    "lines['line'] = [x.replace('l', '') for x in lines['line']]\n",
    "lines['line'] = [int(x) if not x == '' else np.nan for x in lines['line']]\n",
    "lines = lines.sort_values(['id_text', 'line']).reset_index(drop=True)\n",
    "lines.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the new `line` field is not a line number in the traditional sense of the word (this is `label`) but a number used to organize lines in the appropriate order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Save in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = filename[:-4]\n",
    "with open('output/' + filename + '.csv', 'w') as w:\n",
    "    lines.to_csv(w, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4 Manipulate for Document-level Analysis\n",
    "For analyses that use documents in a Document Term Matrix or otherwise a similar type of manipulation is needed. This output may be used in Word2vec, in Topic Modeling and in other types of algorithms. First lemmas and bases are dealt with in the same way as above, section 3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Create Lemmas and Adjust Bases\n",
    "A lemma, [ORACC](http://oracc.org) style, combines Citation Form, GuideWord and POS into a unique reference to one particular lemma in a standard dictionary, as in `lugal[king]N` (Suerian) or `šarru[king]N`. Usually, not all words in a text are lemmatized, because a word may be (partly) broken and/or unknown. Unlemmatized and unlemmatizable words will receive a place-holder lemmatization that consists of the transliteration of the word (instead of the Citation Form), with `NA` as GuideWord and POS, as in `i-bu-x[NA]NA`. Note that `NA` is a string.\n",
    "\n",
    "For Sumerian projects each lemmatized word has a `base` (the word without morphology). For non-lemmatized words a place-holder base is created that consists of the transliteration of the word. If you are not working with Sumerian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words['lemma'] = words['cf'] # first element of lemma is the citation form\n",
    "words['lemma'] = [words['lemma'][i] + '[' + words['gw'][i] \n",
    "                     + ']' + words['pos'][i] \n",
    "                     if not words['lemma'][i] == '' \n",
    "                     else words['form'][i] +'[NA]NA' for i in range(len(words))]\n",
    "words['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in words['lemma'] ]\n",
    "words['base'] = [words['base'][i] if not words['base'][i] == '' \n",
    "                 or words['label'][i] == '' else words['form'][i] \n",
    "                 for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Group by Document\n",
    "In order to group by Document we use the field `id_text` and aggregate `lemma` and `base`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = words.groupby(words['id_text']).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'base': ' '.join,\n",
    "    }).reset_index()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Save in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = filename[:-4]\n",
    "with open('output/' + filename + '.csv', 'w') as w:\n",
    "    lines.to_csv(w, encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Tokenizing\n",
    "Since lemmas do not contain spaces (see above, section 1.8) tokenizing is extremely easy and basically consists of splitting on spaces. Tokenized data are used in Topic Modeling, Word2vec, etc. Tokenizing is also necessary for making a Document Term Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents['tokens'] = documents['lemma'].str.split()\n",
    "documents"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
