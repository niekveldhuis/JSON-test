{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Parser: Lines and Breaks\n",
    "by Niek Veldhuis\n",
    "UC Berkeley\n",
    "\n",
    "July 2017\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The two main differences between `First_JSON_parser.ipynb` and the current notebook are\n",
    "\n",
    "- the ability to parse an entire corpus\n",
    "- recognizing lines\n",
    "- including breaks (as in \"3 lines broken\").\n",
    "\n",
    "Although these features somewhat complicate the code, the basic techniques used are the same.\n",
    "\n",
    "The resulting data file may include various elements of the ORACC data structure. The current code will output a file with the following fields: \n",
    "\n",
    "* id_text\n",
    "* id_line\n",
    "* label\n",
    "* lemma (a sequence of lemmas in a line)\n",
    "* extent\n",
    "* scope\n",
    "* state\n",
    "\n",
    "The fields `extent`, `scope`, and `state` capture the number of missing lines or columns.\n",
    "\n",
    "The selection of fields may be adjusted with standard `Pandas` functions.\n",
    "\n",
    "## Notes\n",
    "\n",
    "This notebook is written for **Python 3.5** with **Pandas 0.19** and **requests 2.12.4**.\n",
    "\n",
    "\n",
    "## Licensing\n",
    "This notebook may be downloaded, used, adapted and distributed without restrictions ([CC0 1.0](https://creativecommons.org/publicdomain/zero/1.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import requests\n",
    "import zipfile\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input List of Text IDs or a project abbreviation\n",
    "Identify a list of text IDs (P, Q, and X numbers) in the directory `input`. The IDs are six-digit P, Q, or X numbers preceded by a project abbreviation in the format 'PROJECT/P######' or 'PROJECT/SUBPROJECT/Q######'. For example:\n",
    "* dcclt/P117395\n",
    "* etcsri/Q001203\n",
    "* rinap/rinap1/Q003421\n",
    "\n",
    "The list should be created with a flat text editor such as Textedit or Emacs, and the filename should end in `.txt`.\n",
    "\n",
    "Alternatively, one may enter the name (abbreviation) of a project or sub-project in [ORACC](http://oracc.org) and pull all the lemmatized data from that project. Note that the script will not automatically pull data from subprojects, they have to be requested separately. Examples:\n",
    "* saao/saa01\n",
    "* aemw/amarna\n",
    "* rimanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename or project abbreviation: saao/sargonletters\n"
     ]
    }
   ],
   "source": [
    "name = input('Filename or project abbreviation: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if name[-4:] == '.txt':\n",
    "    textids = 'text_ids/' + name\n",
    "    with open(textids, 'r') as f:\n",
    "        pqxnos = f.readlines()\n",
    "    pqxnos = [x.strip() for x in pqxnos]  # strip spaces left and right\n",
    "    pqxnos = [x for x in pqxnos if not x == \"\"] # strip empty lines\n",
    "    pqxnos = [x.split()[0] for x in pqxnos] # strip everything after first space\n",
    "#    pqxnos = [x[-7:].upper() for x in pqxnos]\n",
    "    projects = [x[:-8].lower() for x in pqxnos]\n",
    "    projects = list(set(projects))\n",
    "else:\n",
    "    name = name.strip().lower()\n",
    "    projects = [name]\n",
    "    url = \"http://oracc.org/\" + name + \"/corpus.json\"\n",
    "    r = requests.get(url)\n",
    "    corpus = r.json()\n",
    "    pqxnos = list(corpus[\"members\"].keys())\n",
    "    pqxnos = [name + '/' + no for no in pqxnos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create Download Directory and JSON directory\n",
    "For the code, see [Stack Overflow](http://stackoverflow.com/questions/18973418/os-mkdirpath-returns-oserror-when-directory-does-not-exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import errno\n",
    "import os\n",
    "try:\n",
    "    os.mkdir('jsonzip')\n",
    "except OSError as exc:\n",
    "    if exc.errno !=errno.EEXIST:\n",
    "        raise\n",
    "    pass\n",
    "try:\n",
    "    os.mkdir('json')\n",
    "except OSError as exc:\n",
    "    if exc.errno !=errno.EEXIST:\n",
    "        raise\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Download `json.zip`\n",
    "For each project from which files are to be processed download the entire project (all the json files) in `https://github.com/oracc/json`. The file is called `PROJECT.zip` (for instance: `dcclt.zip`). For subprojects the file is called `PROJECT-SUBPROJECT.zip` (for instance `cams-gkab.zip`). \n",
    "\n",
    "For larger projects (such as [DCCLT](http://oracc.org/dcclt)) the `zip` file may be 25Mb or more. Downloading may take some time and it may be necessary to chunk the downloading process. The `iter_content()` function in the `requests` library takes care of that.\n",
    "\n",
    "Although downloading the entire zip file is time consuming, it will make processing the individual files much more efficient and the code is less likely to break due to interruption in connectivity.\n",
    "\n",
    "## Note:\n",
    "It may be better to download the `zip` file from [ORACC](http://oracc.org), where it is available as `http://oracc.org/[PROJECT]/json.zip`. This version is updated when a project is updated. Right now the file seems to be not accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CHUNK = 16 * 1024\n",
    "for project in tqdm.tqdm(projects):\n",
    "    project = project.replace('/', '-')\n",
    "    url = \"https://raw.github.com/oracc/json/master/\" + project + \".zip\"\n",
    "    file = 'jsonzip/' + project + '.zip'\n",
    "    print(\"Downloading \" + url + \" saving as \" + file)\n",
    "    r = requests.get(url)\n",
    "    with open(file, 'wb') as f:\n",
    "        for c in r.iter_content(chunk_size=CHUNK):\n",
    "            f.write(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Extract JSON files from `json.zip`\n",
    "Extract the texts listed in the list of text IDs from the `json.zip`. All files are extracted to a directory called `data/[PROJECT]/json/corpusjson` (for instance `data/dcclt/json/corpusjson`). If the file belongs to a subproject the directory is called `data/[PROJECT]/[SUBPROJECT]/json/corpusjson`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_dir = 'json'\n",
    "files_l = []\n",
    "for no in tqdm.tqdm(pqxnos):\n",
    "    project = no[:-8].lower()\n",
    "    pno = no[-7:].upper()\n",
    "    zip_file = \"jsonzip/\" + project.replace('/', '-') + \".zip\"\n",
    "    with zipfile.ZipFile(zip_file,\"r\") as zip_ref:\n",
    "        file = project + '/corpusjson/' + pno + '.json'\n",
    "        try:\n",
    "            zip_ref.extract(file, target_dir)\n",
    "            files_l.append(file)\n",
    "        except:\n",
    "            print(no + ' is not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Parse JSON files\n",
    "The `parsejson()` function is essentially identical with the that function in `First_JSON_parser.ipynb`, but it fetches more data. The field `word_id` consists of three parts, namely a text ID, line ID, and word ID, in the format `Q000039.76.2` meaning: the second word in line 76 of text object `Q000039`. Note that `76` is not a line number strictly speaking but an object reference within the text object. Things like horizontal rulings, columns, and breaks also get object references. The `word_id` field allows us to put lines together in the proper order.\n",
    "\n",
    "The field `label` is a human-legible label that refers a line or another part of the text; it may look like `o i 23` (obverse column 1 line 23) or `r v 23'` (reverse column 5 line 23 prime). The `label` field is used in online [ORACC](http://oracc.org) editions to indicate line numbers.\n",
    "\n",
    "The fields `extent`, `scope`, and `state` give metatextual data about the condition of the object; they capture the number of broken lines or columns and similar information. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parsejson(text, parameters):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson(JSONobject, parameters)\n",
    "        if \"label\" in JSONobject:\n",
    "            parameters[\"label\"] = JSONobject['label']\n",
    "        if \"f\" in JSONobject:\n",
    "            lemma = JSONobject[\"f\"]\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"]\n",
    "            lemma['label'] = parameters[\"label\"]\n",
    "            lemma[\"id_text\"] = parameters[\"id_text\"]\n",
    "            lemm_l.append(lemma)\n",
    "        if \"strict\" in JSONobject and JSONobject[\"strict\"] == \"1\":\n",
    "            lemma = {key: JSONobject[key] for key in parameters[\"dollar_keys\"]}\n",
    "            lemma[\"id_word\"] = JSONobject[\"ref\"] + \".0\"\n",
    "            lemma[\"id_text\"] = parameters[\"id_text\"]\n",
    "            lemm_l.append(lemma)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Call the Parser Function for Each Textid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemm_l = []\n",
    "parameters = {\"label\": None, \"id_text\": None, \"dollar_keys\" : [\"extent\", \"scope\", \"state\"]}\n",
    "for file in tqdm.tqdm(files_l):\n",
    "    parameters[\"id_text\"] = file.replace('corpusjson/', '')[:-5]\n",
    "    with open(\"json/\" + file) as data_file:\n",
    "        text = json.load(data_file)\n",
    "    try:\n",
    "        parsejson(text, parameters)\n",
    "    except:\n",
    "        print(no + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Structuring\n",
    "### 2.1 Transform the Data into a DataFrame\n",
    "The word_l list is transformed into a Pandas dataframe for further manipulation.\n",
    "\n",
    "For various reasons not all JSON files will have all data types that potentially exist in an [ORACC](http://oracc.org) signature. Only Sumerian words have a `base`, so if your data set has no Sumerian, this column will not exist in the DataFrame.  If a text has no breakage information in the form of `$ 1 line broken` (etc.) the fields `extent`, `scope`, and `state` do not exist. Since such fields are referenced in the code below (sections 2-4) the next cell will check for the existence of each column and create an empty column if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = pd.DataFrame(lemm_l)\n",
    "words = words.fillna('') # replace Missing Values by empty string\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Remove Spaces and Commas from Guide Word and Sense\n",
    "Spaces in Guide Word and Sense may cause trouble in computational methods in tokenization, or when saved in Comma Separated Values format. All spaces and commas are replaced by hyphens or nothing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words['sense'] = [x.replace(' ', '-') for x in words['sense']]\n",
    "words['sense'] = [x.replace(',', '') for x in words['sense']]\n",
    "words['gw'] = [x.replace(' ', '-') for x in words['gw']]\n",
    "words['gw'] = [x.replace(',', '') for x in words['gw']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the resulting DataFrame correspond to the elements of a full [ORACC](http://oracc.org) signature, plus information about text, line, and word ids:\n",
    "* base (Sumerian only)\n",
    "* cf (Citation Form)\n",
    "* cont (continuation of the base; Sumerian only)\n",
    "* epos (Effective Part of Speech)\n",
    "* form (transliteration, omitting all flags such as indication of breakage)\n",
    "* frag (transliteration; including flags)\n",
    "* gdl_utf8 (cuneiform)\n",
    "* gw (Guide Word: main or first translation in standard dictionary)\n",
    "* id_line (a line ID that begins with the six-digit P, Q, or X number of the text)\n",
    "* id_text (six-digit P, Q, or X number)\n",
    "* id_word (word ID that begins with the ID number of the line)\n",
    "* label (traditional line number in the form o ii 2' (obverse column 2 line 2'), etc.)\n",
    "* lang (language code, including sux, sux-x-emegir, sux-x-emesal, akk, akk-x-stdbab, etc)\n",
    "* morph (Morphology; Sumerian only)\n",
    "* norm (Normalization: Akkadian)\n",
    "* norm0 (Normalization: Sumerian)\n",
    "* pos (Part of Speech)\n",
    "* sense (contextual meaning)\n",
    "* sig (full ORACC signature)\n",
    "\n",
    "Not all data elements (columns) are available for all words. Sumerian words never have a `norm`, Akkadian words do not have `norm0`, `base`, `cont`, or `morph`. Most data elements are only present when the word is lemmatized; only `lang`, `form`, `pos`, `id_word`, `id_line`, and `id_text` should always be there. An unlemmatized word has `pos` 'X' (for unknown). Broken words have `pos` 'u' (for 'unlemmatizable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Manipulate for Analysis on Line level\n",
    "For analyses that use a line as unit of analysis (e.g. lines in lexical texts as analyzed in the [Phylogenetics](https://github.com/ErinBecker/digital-humanities-phylogenetics) project) one may need to create lemmas and combine these into lines by using the `id_line` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create Lemmas and Adjust Bases\n",
    "A lemma, [ORACC](http://oracc.org) style, combines Citation Form, GuideWord and POS into a unique reference to one particular lemma in a standard dictionary, as in `lugal[king]N` (Sumerian) or `šarru[king]N`. Usually, not all words in a text are lemmatized, because a word may be (partly) broken and/or unknown. Unlemmatized and unlemmatizable words will receive a place-holder lemmatization that consists of the transliteration of the word (instead of the Citation Form), with `NA` as GuideWord and `NA` as POS, as in `i-bu-x[NA]NA`. Note that `NA` is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words[\"lemma\"] = words.apply(lambda r: (r[\"cf\"] + '[' + r[\"gw\"] + ']' + r[\"pos\"]) \n",
    "                            if r[\"cf\"] != '' else r['form'] + '[NA]NA', axis=1)\n",
    "words['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in words['lemma'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Group by Line\n",
    "In the `words` dataframe each word has a separate row. In order into change this to a line-by-line representation we use the Pandas `.groupby` function, using `id_text`, `id_line` and `label` fields as the sorting arguments. \n",
    "\n",
    "The field `id_line` is created by splitting `id_word` into three elements. The format of `id_word` is `IDtext.line.word`. The middle part, `id_line` is made into an integer so that it can be used to put the lines into their proper order (note that `id_line` is an abstract reference number that indicates the sequence of lines in a text object; `label` is a human-readable line number in the format `o ii 3`: obverse column 2, line 3). \n",
    "\n",
    "The fields that are aggregated are `lemma`, `extent`, `scope`, and `state`. The fields `extent`, `scope`, and `state` represent data on the number of broken lines. For instance, the notation `4 lines missing` in the [ORACC](http://oracc.org) edition will result in `extent = \"4\"`, `scope = \"line\"`, `state = \"missing\"` (note that the value of `extent` is a string and will be `\"n\"` if the number of missing lines or columns is unknown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#words['id_line'] = [wordid[:wordid.rfind('.')+1] for wordid in words['id_word']]\n",
    "words['id_line'] = [int(wordid.split('.')[1]) for wordid in words['id_word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = words.groupby([words['id_text'], words['id_line'], words['label']]).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'extent': ''.join, \n",
    "        'scope': ''.join,\n",
    "        'state': ''.join\n",
    "    }).reset_index()\n",
    "lines        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Save in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = name[:-4]\n",
    "with open('output/' + filename + '.csv', 'w') as w:\n",
    "    lines.to_csv(w, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
