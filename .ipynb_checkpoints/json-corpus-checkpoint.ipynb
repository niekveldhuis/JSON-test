{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing ORACC Data from corpus.json\n",
    "by Niek Veldhuis\n",
    "UC Berkeley\n",
    "\n",
    "February 2017\n",
    "\n",
    "# TODO\n",
    "* check that COFs are treated properly\n",
    "* check that lines that continue into the next line (as in bilinguals) are captured completely.\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Purpose of the code is to download [ORACC](http://oracc.org) JSON files that contain textual data and produce a `.csv` file with the relevant data for use in computational text analysis. This comes in the place of scraping the published `html` (see the [Scrape-ORACC](https://github.com/niekveldhuis/Digital-Assyriology/tree/master/Scrape-Oracc) repo). The JSON files contain all the transliteration and lemmatization data of an ORACC project (metadata are made available in a separate `.json` file). For an introduction to the various ORACC JSON files see the [ORACC Open Data](http://oracc.org/doc/opendata) page.\n",
    "\n",
    "The resulting data file may include various elements of the ORACC data structure. The current code will output a file with the following fields: \n",
    "\n",
    "* id_line\n",
    "* label\n",
    "* lemma\n",
    "* base\n",
    "* extent\n",
    "* scope\n",
    "\n",
    "The fields `extent` and `scope` capture the number of missing lines or columns.\n",
    "\n",
    "The selection of fields may be adjusted with standard `Pandas` functions.\n",
    "\n",
    "## Notes\n",
    "The current version of the script works with the `ijson` library. Documentation for [ijson](https://www.dataquest.io/blog/python-json-tutorial/), unfortunately, is extremely brief. The notebook has been tested, using a list of 106 P, Q, X numbers (in `ob_lists_wood.txt`). Downloading the `.json` files from the ORACC server takes between 30 and 45 seconds. The rest of the script is reasonably fast. With larger lists of text IDs the script will obviously take longer. \n",
    "\n",
    "This notebook is written for **Python 3.5** with **Pandas 0.19** and **ijson 2.3**.\n",
    "\n",
    "The notebook was written for the [Phylogenetics](https://github.com/ErinBecker/digital-humanities-phylogenetics) project with Erin Becker of [Data Carpentry](http://www.datacarpentry.org). The particular data selection and data manipulation performed in this notebook are inspired by the needs of that project (for instance, non-Sumerian words are filtered out). It should be fairly easy to adapt the notebook to the purposes of any other project that wishes to use [ORACC](http://oracc.org) data.\n",
    "\n",
    "## Licensing\n",
    "This notebook may be downloaded, used and adapted without any restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "import ijson\n",
    "import urllib.request\n",
    "import re\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input List of Text IDs\n",
    "Identify a list of text IDs (P, Q, and X numbers) in the directory `input`. The IDs are six-digit P, Q, or X numbers preceded by a project abbreviation in the format 'PROJECT/P######' or 'PROJECT/SUBPROJECT/Q######'. For example:\n",
    "* dcclt/P117395\n",
    "* etcsri/Q001203\n",
    "* rinap/rinap1/Q003421\n",
    "\n",
    "The list should be created with a flat text editor such as Textedit or Emacs, and the filename should end in `.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = input('Filename: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textids = 'text_ids/' + filename\n",
    "with open(textids, 'r') as f:\n",
    "    pqxnos = f.readlines()\n",
    "pqxnos = [x.strip() for x in pqxnos]\n",
    "prefix = [x[:-7] + 'corpusjson/' + x[-7:] for x in pqxnos]\n",
    "pqxnos = [x[-7:] for x in pqxnos]\n",
    "pqxnos[:5], prefix[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse\n",
    "The function `oraccjasonparser()` takes one argument (the **url** of the `.json` file). It looks for the prefix `textid` to retrieve the six-digit P, Q, or X number of the text artifact. Parsing the file sequentially the code looks for the places where a line starts (`'.type' = 'line-start'`) and where a word starts (`'.node' = 'l'`, where `l` is for \"lemma\"). At each level the code will retrieve the relevant data and create a list where each entry is a dictionary that represents a single word. \n",
    "\n",
    "Words not only include lemmatized words, but also unlemmatized and unlemmatizable words (such as breaks).\n",
    "\n",
    "The dictionary includes the keys `id_line` and `id_word` that allow the user to reassemble words and lines in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oraccjasonparser(url):\n",
    "    d = urllib.request.urlopen(url)\n",
    "    parser = ijson.parse(d)\n",
    "    word_l = []\n",
    "    word_d = {}\n",
    "    line_start = False\n",
    "    word_start = False\n",
    "    nonx = False\n",
    "    for prefix, event, value in parser:\n",
    "        if prefix == 'textid':\n",
    "            id_text = value\n",
    "#            print(\"parsing \" + value)\n",
    "        if prefix.endswith('.type'):\n",
    "            if value == 'line-start':\n",
    "                line_start = True\n",
    "            else:\n",
    "                line_start = False\n",
    "        if line_start:\n",
    "            if prefix.endswith('.ref') and not word_start:\n",
    "                id_line = value # id_line is a reference number for a line\n",
    "                                # that includes the id_text (e.g. P123456.49)\n",
    "            if prefix.endswith('.label'):\n",
    "                label = value   # label is a human-readable line number of the format\n",
    "                                # o ii 24' (obverse column 2 line 24')\n",
    "        if prefix.endswith('node'):\n",
    "            if value == 'l':\n",
    "                word_start = True\n",
    "                if not word_d == {}:\n",
    "                    word_l.append(word_d) # append the previous word to the list\n",
    "                word_d = {}               # and start a new dictionary\n",
    "                word_d['id_text'] = id_text # provide each word with appropriate \n",
    "                word_d['id_line'] = id_line # text and line-ID\n",
    "                word_d['label'] = label     # and the line label.\n",
    "            else:\n",
    "                word_start = False\n",
    "        if word_start:\n",
    "            if prefix.endswith('.ref'):\n",
    "                word_d['id_word'] = value\n",
    "            if prefix.endswith('.sig'):\n",
    "                word_d['signature'] = value\n",
    "            if '.f.' in prefix:\n",
    "                category = re.sub('.*\\.', '', prefix) # get element after the last dot of the prefix\n",
    "                word_d[category] = value # copy each element into the dictionary\n",
    "        if prefix.endswith('.type'):\n",
    "            if value == 'nonx':\n",
    "                nonx = True\n",
    "            else:\n",
    "                nonx = False\n",
    "        if nonx:                         # this captures so-called $-lines with information\n",
    "            if prefix.endswith('.ref'):  # about number of broken lines/columns.\n",
    "                id_line = value          # $-lines have their own id_line.\n",
    "            if prefix.endswith('.strict'):\n",
    "                if value == '1':           # select only 'strict' $ lines\n",
    "                    if not word_d == {}:\n",
    "                        word_l.append(word_d)\n",
    "                    word_d = {}\n",
    "                    word_d['id_line'] = id_line\n",
    "                    word_d['id_text'] = id_text\n",
    "                else:\n",
    "                    nonx = False\n",
    "            if prefix.endswith('.extent'): # capture the three elements of strict $ lines\n",
    "                word_d['extent'] = value   # namely extent, scope, and state.\n",
    "            if prefix.endswith('.scope'):\n",
    "                word_d['scope'] = value\n",
    "            if prefix.endswith('.state'):\n",
    "                word_d['state'] = value\n",
    "\n",
    "    word_l.append(word_d)  # make sure that the last word is captured, too.\n",
    "    return(word_l) # return a list of dictionaries, where each entry (dictionary) in\n",
    "                   # the list represents a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the Parser Function for Each Textid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_prefix = \"http://oracc.museum.upenn.edu/\"\n",
    "word_l = []\n",
    "for id_text in tqdm.tqdm(prefix):   # tqdm creates a progress bar\n",
    "    try:\n",
    "        url = url_prefix + id_text + '.json'\n",
    "        word_l.extend(oraccjasonparser(url))\n",
    "    except:\n",
    "        print(url + ' not available') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the Data into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_l)\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Spaces and Commas from Guide Word and Sence\n",
    "Spaces in Guide Word and Sense may cause trouble in computational methods in tokenization, or when saved in Comma Separated Values format. All spaces and commas are replaced by hyphens or nothing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = words.fillna('') # first replace Missing Values by empty string\n",
    "words['sense'] = [x.replace(' ', '-') for x in words['sense']]\n",
    "words['sense'] = [x.replace(',', '') for x in words['sense']]\n",
    "words['gw'] = [x.replace(' ', '-') for x in words['gw']]\n",
    "words['gw'] = [x.replace(',', '') for x in words['gw']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the resulting DataFrame correspond to the elements of a full [ORACC](http://oracc.org) signature, plus information about text, line, and word ids:\n",
    "* base (Sumerian only)\n",
    "* cf (Citation Form)\n",
    "* cont (continuation of the base; Sumerian only)\n",
    "* epos (Effective Part of Speech)\n",
    "* form (transliteration, omitting all flags such as indication of breakage)\n",
    "* gw (Guide Word: main or first translation in standard dictionary)\n",
    "* id_line (a line ID that begins with the six-digit P, Q, or X number of the text)\n",
    "* id_text (six-digit P, Q, or X number)\n",
    "* id_word (word ID that begins with the ID number of the line)\n",
    "* label (traditional line number in the form o ii 2' (obverse column 2 line 2'), etc.)\n",
    "* lang (language code, including sux, sux-x-emegir, sux-x-emesal, akk, akk-x-stdbab, etc)\n",
    "* morph (Morphology; Sumerian only)\n",
    "* norm (Normalization: Akkadian)\n",
    "* norm0 (Normalization: Sumerian)\n",
    "* pos (Part of Speech)\n",
    "* sense (contextual meaning)\n",
    "* signature (full ORACC signature)\n",
    "\n",
    "Not all data elements (columns) are available for all words. Sumerian words never have a `norm`, Akkadian words do not have `norm0`, `base`, `cont`, or `morph`. Most data elements are only present when the word is lemmatized; only `lang`, `form`, `pos`, `id_word`, `id_line`, and `id_text` should always be there. An unlemmatized word has `pos` 'X' (for unknown). Broken words have `pos` 'u' (for 'unlemmatizable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#words[words['label'] == '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate\n",
    "The columns may be manipulated with standard Pandas methods to create the desired output. By way of example, the following code will create a column `lemma` with the format **cf[gw]pos** (for instance **lugal[king]N**). For words that have no lemmatization `lemma` equals `form`. Only Sumerian words are allowed (and thus `lang` can be omitted) and in addition to the column `lemma` the column `base` is preserved; words that have no lemmatization take `form` as their base. Words and bases are concatenated to lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove  non-Sumerian words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang = ['sux', ''] # note that 'lang' is empty in entries that indicate damage\n",
    "words = words.loc[words['lang'].str[:3].isin(lang)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lemma Column and Adjust Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words['lemma'] = words['cf'] # first element of lemma is the citation form\n",
    "words['lemma'] = [words['lemma'][i] + '[' + \n",
    "                  words['gw'][i] + ']' +\n",
    "                  words['pos'][i] if not words['lemma'][i] == ''\n",
    "                  else words['form'][i] +'[NA]NA' for i in range(len(words))]\n",
    "words['lemma'] = [lemma if not lemma == '[NA]NA' else '' for lemma in words['lemma'] ]\n",
    "words['base'] = [words['base'][i] if not words['base'][i] == '' \n",
    "                 or words['label'][i] == '' else words['form'][i] \n",
    "                 for i in range(len(words))]\n",
    "#words['label'] = [label if not label == '' else 'none' for label in words['label']]\n",
    "lemmas = words[['lemma', 'base', 'id_text', 'id_line', 'id_word', 'label', 'extent', 'scope']]\n",
    "lemmas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = words.groupby([words['id_line'], words['label']]).agg({\n",
    "        'lemma': ' '.join,\n",
    "        'base': ' '.join,\n",
    "        'extent': ''.join, \n",
    "        'scope': ''.join}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = lines[['id_line', 'label', 'lemma', 'base', 'extent', 'scope']]\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = filename[:-4]\n",
    "with open('output/' + filename + '.csv', 'w') as w:\n",
    "    lines.to_csv(w, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
